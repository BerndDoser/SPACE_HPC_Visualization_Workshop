{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDLEcO17VV5n"
      },
      "source": [
        "# Spherinator & HiPSter\n",
        "\n",
        "This notebook demonstrates how to train a Spherinator model using the Illustris TNG dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "k3DoeTkKD2Pb"
      },
      "source": [
        ">[Spherinator & HiPSter](#scrollTo=VDLEcO17VV5n)\n",
        "\n",
        ">[Part 1: Spherinator - The Training](#scrollTo=cr4MS5QWElnI)\n",
        "\n",
        ">>[Download data](#scrollTo=rAGt3xTXVV5o)\n",
        "\n",
        ">>[Visualize training data](#scrollTo=q0B8v-QzVV5p)\n",
        "\n",
        ">>[Define the model](#scrollTo=0dn14y9eVV5p)\n",
        "\n",
        ">>[Define the Parquet Data Module](#scrollTo=xlmTZ74vVV5q)\n",
        "\n",
        ">>[Setup the PyTorch Lightning Trainer and start the fitting process](#scrollTo=is4LBLalH22a)\n",
        "\n",
        ">>[Export the trained model to ONNX](#scrollTo=g2j8u6nyVV5q)\n",
        "\n",
        ">[Part 2: HiPSter - The Inference](#scrollTo=emV4xrtPC12n)\n",
        "\n",
        ">>[Catalog as VOTable](#scrollTo=U3R61t0CGvKK)\n",
        "\n",
        ">>[Visualize HiPS tiles and catalog using Aladin-Lite](#scrollTo=pTeGzP8cIcF8)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr4MS5QWElnI"
      },
      "source": [
        "# Part 1: Spherinator - The Training\n",
        "\n",
        "First we have to install the Spherinator package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkwhZV0LVYVB"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    %pip -q install git+https://github.com/HITS-AIN/Spherinator\n",
        "import spherinator\n",
        "print(spherinator.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAGt3xTXVV5o"
      },
      "source": [
        "## Download data\n",
        "\n",
        "For a small test data set we use 200 selected synthetic\n",
        "[SKIRT](https://www.tng-project.org/data/docs/specifications/#sec5l) images the Illustris TNG100-1\n",
        "simulation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E4d_7LdVV5p"
      },
      "source": [
        "In the parquet schema the metadata `simulation`, `snapshot`, and `subhalo_id` are stored in the\n",
        "`metadata` column. The `data` column contains the actual data as a list. The actual shape `(3, 128,\n",
        "128)` is stored in the schema metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wyn4xuud-gER"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XxPUdoKpZCNKnh3X725V1fjQN8pXJnS2\n",
            "To: /home/doserbd/git/SPACE_HPC_Visualization_Workshop/colab/illustris.parquet\n",
            "100%|██████████| 76.2M/76.2M [00:51<00:00, 1.49MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'illustris.parquet'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pip -q install --upgrade gdown\n",
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1XxPUdoKpZCNKnh3X725V1fjQN8pXJnS2', 'illustris.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra7cLGAfVV5p"
      },
      "outputs": [],
      "source": [
        "import pyarrow.dataset as ds\n",
        "\n",
        "dataset = ds.dataset(\"illustris.parquet\", format=\"parquet\")\n",
        "dataset.schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UppkMX33Yzf"
      },
      "outputs": [],
      "source": [
        "df = dataset.to_table().to_pandas()\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0B8v-QzVV5p"
      },
      "source": [
        "## Visualize training data\n",
        "\n",
        "To get an impression of the data, we can visualize the first 50 images of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RlE2AQIVV5p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig, axes = plt.subplots(5, 5, figsize=(15, 15))\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    data = np.array(df[\"data\"][i]).reshape(3, 128, 128).transpose(1, 2, 0) * 255\n",
        "    image = Image.fromarray(data.astype(np.uint8), \"RGB\")\n",
        "    ax.imshow(image)\n",
        "    ax.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dn14y9eVV5p"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "As model we use a `VariationalAutoencoder` with a convolutional network as encoder and decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuXW4KSPVV5p"
      },
      "outputs": [],
      "source": [
        "import spherinator.models as sm\n",
        "\n",
        "model = sm.VariationalAutoencoder(\n",
        "    encoder=sm.ConvolutionalEncoder2D(\n",
        "        input_dim=[3, 128, 128],\n",
        "        output_dim=128,\n",
        "        cnn_layers=[\n",
        "            sm.ConsecutiveConv2DLayer(\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "                out_channels=[16, 20, 24],\n",
        "            ),\n",
        "            sm.ConsecutiveConv2DLayer(\n",
        "                kernel_size=4,\n",
        "                stride=2,\n",
        "                padding=0,\n",
        "                out_channels=[64, 128],\n",
        "            ),\n",
        "        ],\n",
        "    ),\n",
        "    decoder=sm.ConvolutionalDecoder2D(\n",
        "        input_dim=3,\n",
        "        output_dim=[3, 128, 128],\n",
        "        cnn_input_dim=[128, 28, 28],\n",
        "        cnn_layers=[\n",
        "            sm.ConsecutiveConvTranspose2DLayer(\n",
        "                kernel_size=5,\n",
        "                stride=2,\n",
        "                padding=0,\n",
        "                out_channels=[64],\n",
        "            ),\n",
        "            sm.ConsecutiveConvTranspose2DLayer(\n",
        "                kernel_size=6,\n",
        "                stride=2,\n",
        "                padding=0,\n",
        "                out_channels=[24],\n",
        "            ),\n",
        "            sm.ConsecutiveConvTranspose2DLayer(\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "                out_channels=[20, 16, 3],\n",
        "                activation=None,\n",
        "            ),\n",
        "        ],\n",
        "    ),\n",
        "    z_dim=3,\n",
        "    beta=1.0e-4,\n",
        "    encoder_out_dim=128,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlmTZ74vVV5q"
      },
      "source": [
        "## Define the Parquet Data Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt5y1I1gVV5q"
      },
      "outputs": [],
      "source": [
        "from spherinator.data import ParquetDataModule\n",
        "\n",
        "datamodule = ParquetDataModule(\n",
        "    data_directory=\"illustris.parquet\",\n",
        "    data_column=\"data\",\n",
        "    normalize=\"minmax\",\n",
        "    batch_size=256,\n",
        "    num_workers=4,\n",
        "    shuffle=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is4LBLalH22a"
      },
      "source": [
        "## Setup the PyTorch Lightning Trainer and start the fitting process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTX2zf2iVV5q"
      },
      "outputs": [],
      "source": [
        "from lightning.pytorch import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=10,\n",
        "    accelerator=\"auto\",\n",
        "    precision=\"16-mixed\",\n",
        ")\n",
        "trainer.fit(model, datamodule=datamodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2j8u6nyVV5q"
      },
      "source": [
        "## Export the trained model to ONNX\n",
        "\n",
        "- The model include the variational autoencoder part, which is not needed for the Inference.\n",
        "- We export only the encoder and the decoder part of the model.\n",
        "- Dynamic axes are used to allow for variable input sizes.\n",
        "- Unique names are used for the input and output tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp3qCFpDVV5q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "onnx = torch.onnx.export(\n",
        "    model.variational_encoder,\n",
        "    torch.randn(2, 3, 128, 128, device=\"cpu\"),\n",
        "    dynamic_axes={\"x\": {0: \"batch\"}},\n",
        "    input_names=[\"x\"],\n",
        "    output_names=[\"coord\", \"scale\"],\n",
        "    dynamo=True,\n",
        ")\n",
        "onnx.optimize()\n",
        "onnx.save(\"encoder.onnx\")\n",
        "\n",
        "onnx = torch.onnx.export(\n",
        "    model.decoder,\n",
        "    torch.randn(2, 3, device=\"cpu\"),\n",
        "    dynamic_axes={\"input\": {0: \"batch\"}},\n",
        "    dynamo=True,\n",
        ")\n",
        "onnx.optimize()\n",
        "onnx.save(\"decoder.onnx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emV4xrtPC12n"
      },
      "source": [
        "# Part 2: HiPSter - The Inference\n",
        "\n",
        "In the second part we use HiPSter to perform inference on the Illustris TNG simulation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPR-39oeFgXy"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    %pip -q install git+https://github.com/HITS-AIN/HiPSter\n",
        "\n",
        "import hipster\n",
        "print(hipster.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Wc41qIJFThM"
      },
      "outputs": [],
      "source": [
        "hipster.HiPSGenerator(\n",
        "    decoder=hipster.Inference(\"decoder.onnx\"),\n",
        "    image_maker=hipster.ImagePlotter(),\n",
        "    max_order=4,\n",
        "    hips_path=\"output/illustris\",\n",
        ").execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3R61t0CGvKK"
      },
      "source": [
        "## Catalog as VOTable\n",
        "\n",
        "A VOTable (or HIPS catalog) can be used to visualize where an input image is located in the latent\n",
        "space. The `hipster.VOTableGenerator` take all images from the `data_directory` and use the\n",
        "`hipster.Inference` class to encode them into the latent space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z37R-5InG2l6"
      },
      "outputs": [],
      "source": [
        "hipster.VOTableGenerator(\n",
        "    encoder=hipster.Inference(\"encoder.onnx\"),\n",
        "    data_directory=\"illustris.parquet\",\n",
        "    output_file=\"illustris.vot\",\n",
        "    root_path=\"output\",\n",
        ").execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTeGzP8cIcF8"
      },
      "source": [
        "## Visualize HiPS tiles and catalog using Aladin-Lite\n",
        "\n",
        "The HiPS tiles and catalogs can be visualized using\n",
        "[Aladin-Lite](https://github.com/cds-astro/aladin-lite). Here we use\n",
        "[ipyaladin](https://github.com/cds-astro/ipyaladin), which allows to integrate Aladin-Lite in\n",
        "Jupyter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSPET1WSIjgW"
      },
      "outputs": [],
      "source": [
        "%pip -q install ipyaladin\n",
        "\n",
        "from ipyaladin import Aladin\n",
        "\n",
        "aladin = Aladin(survey=\"output/illustris\", fov=180, show_fullscreen_control=False)\n",
        "aladin.add_catalog_from_URL(\"output/illustris.vot\", {\"source_size\": 5, \"color\": \"red\"})\n",
        "aladin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gdown\n",
        "\n",
        "if not os.path.exists(\"illustris_full_trained\"):\n",
        "    gdown.download_folder(\n",
        "        \"https://drive.google.com/drive/folders/1BYLoV83Jb9IpIi2YHYUb2Wokubo9Tn3M\",\n",
        "        output=\"illustris_full_trained\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hipster.HiPSGenerator(\n",
        "    decoder=hipster.Inference(\"illustris_full_trained/decoder.onnx\"),\n",
        "    image_maker=hipster.ImagePlotter(),\n",
        "    max_order=4,\n",
        "    hips_path=\"output/illustris\",\n",
        ").execute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ipyaladin import Aladin\n",
        "\n",
        "aladin = Aladin(survey=\"output/illustris\", fov=180, show_fullscreen_control=False)\n",
        "aladin"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
