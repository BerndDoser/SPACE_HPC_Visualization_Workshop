{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for Spherinator Training using Gaia XP DR3\n",
    "\n",
    "## Download the Gaia DR3 XP data\n",
    "\n",
    "For the demo only use the first of 3386 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc -P ./data/gaia/csv https://cdn.gea.esac.esa.int/Gaia/gdr3/Spectroscopy/xp_continuous_mean_spectrum/XpContinuousMeanSpectrum_000000-003111.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pest import GaiaConverter\n",
    "\n",
    "gaia_converter = GaiaConverter(\n",
    "    with_flux_error=True,\n",
    "    number_of_workers=1,\n",
    ")\n",
    "gaia_converter.convert_all(\"data/gaia/csv\", \"data/gaia/parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training data\n",
    "\n",
    "To get an impression of the data, we can visualize the first 10 images of the training data. The\n",
    "data is normalized to be in the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import hipster\n",
    "from PIL import Image\n",
    "\n",
    "dataset = ds.dataset(\"data/gaia/parquet\", format=\"parquet\")\n",
    "print(\"Number of spectra:\", dataset.count_rows())\n",
    "\n",
    "df = dataset.to_table().to_pandas()\n",
    "data = df[\"flux\"][1]\n",
    "spectrum = hipster.SpectrumPlotter(\n",
    "    wavelengths=hipster.Range(336, 1021, 2),\n",
    "    # ylim=(0, 1),\n",
    "    figsize_in_pixel=512,\n",
    ")(data)\n",
    "\n",
    "image = Image.fromarray(spectrum)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spherinator.models as sm\n",
    "\n",
    "model = sm.VariationalAutoencoder(\n",
    "    encoder=sm.ConvolutionalEncoder1D(\n",
    "        input_dim=[1, 343],\n",
    "        output_dim=128,\n",
    "        cnn_layers=[\n",
    "            sm.ConsecutiveConv1DLayer(\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                num_layers=5,\n",
    "                base_channel_number=16,\n",
    "                channel_increment=4,\n",
    "            ),\n",
    "            sm.ConsecutiveConv1DLayer(\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                num_layers=1,\n",
    "                base_channel_number=64,\n",
    "            ),\n",
    "            sm.ConsecutiveConv1DLayer(\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                num_layers=1,\n",
    "                base_channel_number=96,\n",
    "            ),\n",
    "            sm.ConsecutiveConv1DLayer(\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                num_layers=1,\n",
    "                base_channel_number=128,\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    decoder=sm.ConvolutionalDecoder1D(\n",
    "        input_dim=3,\n",
    "        output_dim=[1, 343],\n",
    "        cnn_input_dim=[128, 36],\n",
    "        cnn_layers=[\n",
    "            sm.ConsecutiveConvTranspose1DLayer(\n",
    "                kernel_size=6,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                out_channels_list=[96],\n",
    "            ),\n",
    "            sm.ConsecutiveConvTranspose1DLayer(\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                out_channels_list=[64],\n",
    "            ),\n",
    "            sm.ConsecutiveConvTranspose1DLayer(\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                out_channels_list=[32],\n",
    "            ),\n",
    "            sm.ConsecutiveConvTranspose1DLayer(\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                out_channels_list=[28, 24, 20, 16, 1],\n",
    "                activation=None,\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    z_dim=3,\n",
    "    beta=1.0e-4,\n",
    "    encoder_out_dim=128,\n",
    ")\n",
    "# _ = model(model.example_input_array)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> use every pytorch module\n",
    "-> explain the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spherinator.data as sd\n",
    "\n",
    "datamodule = sd.ParquetDataModule(\n",
    "    data_directory=\"data/gaia/parquet\",\n",
    "    data_column=\"flux\",\n",
    "    normalize=\"minmax\",\n",
    "    batch_size=2048,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    ")\n",
    "# datamodule.setup(\"fit\")\n",
    "# print(f\"Number of training items: {len(datamodule.data_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    ")\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the trained model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "onnx = torch.onnx.export(\n",
    "    model.variational_encoder,\n",
    "    torch.randn(1, 1, 343, device=\"cpu\"),\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}},\n",
    "    dynamo=True,\n",
    ")\n",
    "onnx.optimize()\n",
    "onnx.save(\"data/gaia/models/encoder.onnx\")\n",
    "\n",
    "onnx = torch.onnx.export(\n",
    "    model.decoder,\n",
    "    torch.randn(1, 3, device=\"cpu\"),\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}},\n",
    "    dynamo=True,\n",
    ")\n",
    "onnx.optimize()\n",
    "onnx.save(\"data/gaia/models/decoder.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the ONNX model with netron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q netron\n",
    "import netron\n",
    "netron.start('data/gaia/models/decoder.onnx', 8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Lightning Commnd Line Interface (CLI)\n",
    "\n",
    "Start the training using a unique [yaml config-file](./configs/spherinator/gaia_vae_8.yaml) in reproducible mode.\n",
    "\n",
    "```bash\n",
    "spherinator fit --c configs/spherinator/gaia_vae_8.yaml\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
