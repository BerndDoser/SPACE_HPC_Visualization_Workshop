{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for Spherinator Training using Gaia XP DR3\n",
    "\n",
    "## Download the Gaia DR3 XP data\n",
    "\n",
    "For the demo only use the first of 3386 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘./data/gaia/csv/XpContinuousMeanSpectrum_000000-003111.csv.gz’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc -P ./data/gaia/csv https://cdn.gea.esac.esa.int/Gaia/gdr3/Spectroscopy/xp_continuous_mean_spectrum/XpContinuousMeanSpectrum_000000-003111.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "- Conversion of files from csv to parquet\n",
    "- Normalization of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files to convert\n",
      "File data/gaia/parquet/XpContinuousMeanSpectrum_000000-003111.parquet already exists, skipping\n"
     ]
    }
   ],
   "source": [
    "from pest import GaiaConverter\n",
    "\n",
    "gaia_converter = GaiaConverter(\n",
    "    with_flux_error=True,\n",
    "    number_of_workers=1,\n",
    ")\n",
    "gaia_converter.convert_all(\"data/gaia/csv\", \"data/gaia/parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doserbd/git/Gaia/.venv/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/home/doserbd/git/Gaia/.venv/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import spherinator.models as sm\n",
    "\n",
    "model = sm.VariationalAutoencoder(\n",
    "    encoder=sm.ConvolutionalEncoder1D(\n",
    "        input_dim=[1, 343],\n",
    "        output_dim=128,\n",
    "        cnn_layers=[\n",
    "            sm.ConsecutiveConv1DLayer(\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                num_layers=5,\n",
    "                base_channel_number=16,\n",
    "                channel_increment=4,\n",
    "            ),\n",
    "            sm.ConsecutiveConv1DLayer(\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                num_layers=1,\n",
    "                base_channel_number=64,\n",
    "            ),\n",
    "            sm.ConsecutiveConv1DLayer(\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                num_layers=1,\n",
    "                base_channel_number=96,\n",
    "            ),\n",
    "            sm.ConsecutiveConv1DLayer(\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                num_layers=1,\n",
    "                base_channel_number=128,\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    decoder=sm.ConvolutionalDecoder1D(\n",
    "        input_dim=3,\n",
    "        output_dim=[1, 343],\n",
    "        cnn_input_dim=[128, 36],\n",
    "        cnn_layers=[\n",
    "            sm.ConsecutiveConvTranspose1DLayer(\n",
    "                kernel_size=6,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                out_channels_list=[96],\n",
    "            ),\n",
    "            sm.ConsecutiveConvTranspose1DLayer(\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                out_channels_list=[64],\n",
    "            ),\n",
    "            sm.ConsecutiveConvTranspose1DLayer(\n",
    "                kernel_size=5,\n",
    "                stride=2,\n",
    "                padding=0,\n",
    "                out_channels_list=[32],\n",
    "            ),\n",
    "            sm.ConsecutiveConvTranspose1DLayer(\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                out_channels_list=[28, 24, 20, 16, 1],\n",
    "                activation=None,\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    z_dim=3,\n",
    "    beta=1.0e-4,\n",
    "    encoder_out_dim=128,\n",
    ")\n",
    "# _ = model(model.example_input_array)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spherinator.data as sd\n",
    "\n",
    "datamodule = sd.ParquetDataModule(\n",
    "    data_directory=\"data/gaia/parquet\",\n",
    "    data_column=\"flux\",\n",
    "    normalize=\"minmax\",\n",
    "    batch_size=2048,\n",
    "    num_workers=4,\n",
    "    shuffle=True,\n",
    ")\n",
    "# datamodule.setup(\"fit\")\n",
    "# print(f\"Number of training items: {len(datamodule.data_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/doserbd/git/Gaia/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type                   | Params | Mode  | In sizes    | Out sizes       \n",
      "--------------------------------------------------------------------------------------------------------\n",
      "0 | encoder             | ConvolutionalEncoder1D | 710 K  | train | [1, 1, 343] | [1, 128]        \n",
      "1 | decoder             | ConvolutionalDecoder1D | 150 K  | train | [1, 3]      | [1, 1, 343]     \n",
      "2 | variational_encoder | VariationalEncoder     | 710 K  | train | [1, 1, 343] | [[1, 3], [1, 1]]\n",
      "3 | reconstruction_loss | MSELoss                | 0      | train | ?           | ?               \n",
      "--------------------------------------------------------------------------------------------------------\n",
      "861 K     Trainable params\n",
      "0         Non-trainable params\n",
      "861 K     Total params\n",
      "3.447     Total estimated model params size (MB)\n",
      "83        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/doserbd/git/Gaia/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (43) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cba6094e0e045aabe91e77f231c2933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    ")\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the trained model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doserbd/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/utils.py:517: UserWarning: f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, and autograd_inlining are not supported for dynamo export at the moment.\n",
      "  warnings.warn(\n",
      "/home/doserbd/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/utils.py:534: UserWarning: Specified dynamic axes is not supported for dynamo export at the moment.\n",
      "  warnings.warn(\n",
      "/home/doserbd/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OnnxExporterError",
     "evalue": "Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/exporter.py:1509\u001b[39m, in \u001b[36mdynamo_export\u001b[39m\u001b[34m(model, export_options, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m   1503\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1504\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExporter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1505\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresolved_export_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/exporter.py:1236\u001b[39m, in \u001b[36mExporter.export\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1231\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.options.diagnostic_context, decomposition_skip.enable_decomposition_skips(\n\u001b[32m   1232\u001b[39m     \u001b[38;5;28mself\u001b[39m.options\n\u001b[32m   1233\u001b[39m ), torch._dynamo.config.patch(\n\u001b[32m   1234\u001b[39m     dataclasses.asdict(DEFAULT_EXPORT_DYNAMO_CONFIG)\n\u001b[32m   1235\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1236\u001b[39m     graph_module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfx_tracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# TODO: Defer `import onnxscript` out of `import torch` path\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/103764\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/fx/dynamo_graph_extractor.py:233\u001b[39m, in \u001b[36mDynamoExport.generate_fx\u001b[39m\u001b[34m(self, options, model, model_args, model_kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m updated_model_args = \u001b[38;5;28mself\u001b[39m.input_adapter.apply(\n\u001b[32m    230\u001b[39m     *model_args, model=model, **model_kwargs\n\u001b[32m    231\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre_export_passes\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_model_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/fx/dynamo_graph_extractor.py:243\u001b[39m, in \u001b[36mDynamoExport.pre_export_passes\u001b[39m\u001b[34m(self, options, original_model, fx_module, fx_module_args)\u001b[39m\n\u001b[32m    235\u001b[39m \u001b[38;5;129m@_beartype\u001b[39m.beartype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpre_export_passes\u001b[39m(\n\u001b[32m    237\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m     fx_module_args: Sequence[Any],\n\u001b[32m    242\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexporter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommon_pre_export_passes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_module_args\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/exporter.py:1548\u001b[39m, in \u001b[36mcommon_pre_export_passes\u001b[39m\u001b[34m(options, original_model, fx_module, fx_module_args)\u001b[39m\n\u001b[32m   1546\u001b[39m \u001b[38;5;66;03m# ONNX does not support views and mutations.\u001b[39;00m\n\u001b[32m   1547\u001b[39m \u001b[38;5;66;03m# Functionalize to get a semantically equivalent graph without mutations.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1548\u001b[39m module = \u001b[43mpasses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFunctionalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiagnostic_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_dynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdynamic_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_fake_constant\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfake_context\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfx_module_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[38;5;66;03m# Input mutations are detected and distilled after `Functionalize` pass.\u001b[39;00m\n\u001b[32m   1556\u001b[39m \u001b[38;5;66;03m# Remove them since ONNX inference does not need them.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py:152\u001b[39m, in \u001b[36mdiagnose_call.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_and_raise_if_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiag\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/diagnostics/infra/context.py:369\u001b[39m, in \u001b[36mDiagnosticContext.log_and_raise_if_error\u001b[39m\u001b[34m(self, diagnostic)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m diagnostic.source_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m diagnostic.source_exception\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m RuntimeErrorWithDiagnostic(diagnostic)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/diagnostics/infra/decorator.py:136\u001b[39m, in \u001b[36mdiagnose_call.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     return_values = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m diag.log_section(logging.INFO, \u001b[33m\"\u001b[39m\u001b[33mReturn values\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/fx/_pass.py:278\u001b[39m, in \u001b[36mTransform.run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    276\u001b[39m     old_tabular = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# Gather graph information after transform.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/fx/passes/functionalization.py:124\u001b[39m, in \u001b[36mFunctionalize._run\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m proxy_tensor.maybe_disable_fake_tensor_mode(), fake_mode:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     graph_module = \u001b[43mproxy_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunctionalized_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecomposition_table\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtracing_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtracing_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_allow_non_fake_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_allow_fake_constant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mallow_fake_constant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmaybe_fake_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# Rename placeholder targets to match the original module's signature since\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# We don't want to map forward(x, y, z) to forward(arg0, arg1, arg2).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:1421\u001b[39m, in \u001b[36mmake_fx.<locals>.wrapped\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m   1419\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m   1420\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args):\n\u001b[32m-> \u001b[39m\u001b[32m1421\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_fx_tracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:1367\u001b[39m, in \u001b[36m_MakefxTracer.trace\u001b[39m\u001b[34m(self, f, *args)\u001b[39m\n\u001b[32m   1366\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_modes_from_inputs(f, args):\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_trace_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:1354\u001b[39m, in \u001b[36m_MakefxTracer._trace_inner\u001b[39m\u001b[34m(self, f, *args)\u001b[39m\n\u001b[32m   1351\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m decompose(\u001b[38;5;28mself\u001b[39m.decomposition_table), \u001b[38;5;28mself\u001b[39m.fake_tensor_mode, \u001b[38;5;28mself\u001b[39m.python_dispatcher_mode, \u001b[38;5;28mself\u001b[39m.proxy_function_mode, \\\n\u001b[32m   1352\u001b[39m      \u001b[38;5;28mself\u001b[39m.proxy_mode.sym_mode, \u001b[38;5;28mself\u001b[39m.torch_fn_metadata_mode, \\\n\u001b[32m   1353\u001b[39m      \u001b[38;5;28mself\u001b[39m.proxy_mode, disable_autocast_cache(), _set_make_fx_tracer(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1354\u001b[39m     t = \u001b[43mdispatch_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwrap_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtracer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfx_tracer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mphs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;66;03m# TODO: kind of a bad way to do it, should maybe figure out a better way\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/_compile.py:31\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:642\u001b[39m, in \u001b[36mdispatch_trace\u001b[39m\u001b[34m(root, tracer, concrete_args)\u001b[39m\n\u001b[32m    636\u001b[39m \u001b[38;5;129m@torch\u001b[39m._disable_dynamo\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdispatch_trace\u001b[39m(\n\u001b[32m    638\u001b[39m         root: Union[torch.nn.Module, Callable],\n\u001b[32m    639\u001b[39m         tracer: Tracer,\n\u001b[32m    640\u001b[39m         concrete_args: Optional[Tuple[Any, ...]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    641\u001b[39m ) -> GraphModule:\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m     graph = \u001b[43mtracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    643\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx_passes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdedupe_symint_uses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dedupe_symints\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    599\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/fx/_symbolic_trace.py:822\u001b[39m, in \u001b[36mTracer.trace\u001b[39m\u001b[34m(self, root, concrete_args)\u001b[39m\n\u001b[32m    816\u001b[39m         _autowrap_check(\n\u001b[32m    817\u001b[39m             patcher, module.\u001b[34m__dict__\u001b[39m, \u001b[38;5;28mself\u001b[39m._autowrap_function_ids\n\u001b[32m    818\u001b[39m         )\n\u001b[32m    819\u001b[39m     \u001b[38;5;28mself\u001b[39m.create_node(\n\u001b[32m    820\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    821\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m         (\u001b[38;5;28mself\u001b[39m.create_arg(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m),),\n\u001b[32m    823\u001b[39m         {},\n\u001b[32m    824\u001b[39m         type_expr=fn.\u001b[34m__annotations__\u001b[39m.get(\u001b[33m\"\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    825\u001b[39m     )\n\u001b[32m    827\u001b[39m \u001b[38;5;28mself\u001b[39m.submodule_paths = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:660\u001b[39m, in \u001b[36mwrap_key.<locals>.wrapped\u001b[39m\u001b[34m(*proxies)\u001b[39m\n\u001b[32m    658\u001b[39m     track_tensor_tree(flat_tensors, flat_proxies, constant=\u001b[38;5;28;01mNone\u001b[39;00m, tracer=tracer)\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m out = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    661\u001b[39m out = pytree.tree_map_only(\n\u001b[32m    662\u001b[39m     torch.Tensor,\n\u001b[32m    663\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m t: get_proxy_slot(t, tracer, t, \u001b[38;5;28;01mlambda\u001b[39;00m x: x.proxy),\n\u001b[32m    664\u001b[39m     out\n\u001b[32m    665\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(arg0)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/fx/passes/functionalization.py:87\u001b[39m, in \u001b[36mFunctionalize._functionalize.<locals>.wrapped\u001b[39m\u001b[34m(*inputs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     out = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs_functional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/fx/passes/_utils.py:31\u001b[39m, in \u001b[36mwrap_graph_module_for_node_meta_preservation.<locals>.wrapped\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m fx_traceback.preserve_node_meta():\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInterpreter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/fx/interpreter.py:146\u001b[39m, in \u001b[36mInterpreter.run\u001b[39m\u001b[34m(self, initial_env, enable_io_processing, *args)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m.env[node] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/fx/interpreter.py:203\u001b[39m, in \u001b[36mInterpreter.run_node\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/fx/interpreter.py:275\u001b[39m, in \u001b[36mInterpreter.call_function\u001b[39m\u001b[34m(self, target, args, kwargs)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;66;03m# Execute the function and return the result\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/_ops.py:667\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self_, *args, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(self_, *args, **kwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[32m    666\u001b[39m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/fx/experimental/proxy_tensor.py:705\u001b[39m, in \u001b[36mTorchFunctionMetadataMode.__torch_function__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28mself\u001b[39m.tracer.torch_fn_counts[func] = \u001b[38;5;28mself\u001b[39m.tracer.torch_fn_counts.get(func, \u001b[32m0\u001b[39m) + \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/_ops.py:667\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self_, *args, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(self_, *args, **kwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[32m    666\u001b[39m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: false INTERNAL ASSERT FAILED at \"aten/src/ATen/RegisterFunctionalization_2.cpp\":8054, please report a bug to PyTorch. mutating a non-functional tensor with a functional tensor is not allowed. Please ensure that all of your inputs are wrapped inside of a functionalize() call.\n\nWhile executing %_native_batch_norm_legit : [num_users=3] = call_function[target=torch.ops.aten._native_batch_norm_legit.default](args = (%convolution, %_param_constant2, %_param_constant3, %_tensor_constant1, %_tensor_constant2, True, 0.1, 1e-05), kwargs = {})\nOriginal traceback:\n  File \"/home/doserbd/git/Gaia/.venv/lib/python3.12/site-packages/spherinator/models/variational_autoencoder.py\", line 47, in forward\n    x = self.encoder(x)\n  File \"/home/doserbd/git/Gaia/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/doserbd/git/Gaia/.venv/lib/python3.12/site-packages/spherinator/models/convolutional_encoder_1d.py\", line 51, in forward\n    x = self.cnn(x)\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOnnxExporterError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m onnx = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvariational_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m343\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m onnx.optimize()\n\u001b[32m     10\u001b[39m onnx.save(\u001b[33m\"\u001b[39m\u001b[33mencoder.onnx\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/utils.py:539\u001b[39m, in \u001b[36mexport\u001b[39m\u001b[34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining, dynamo)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;66;03m# TODO: expose more ExportOptions?\u001b[39;00m\n\u001b[32m    538\u001b[39m export_options = torch.onnx.ExportOptions(dynamic_shapes=dynamic_shapes)\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m onnx_program = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43monnx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdynamo_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexport_options\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    543\u001b[39m     onnx_program.save(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/Gaia/.venv/lib/python3.12/site-packages/torch/onnx/_internal/exporter.py:1520\u001b[39m, in \u001b[36mdynamo_export\u001b[39m\u001b[34m(model, export_options, *model_args, **model_kwargs)\u001b[39m\n\u001b[32m   1512\u001b[39m resolved_export_options.diagnostic_context.dump(sarif_report_path)\n\u001b[32m   1513\u001b[39m message = (\n\u001b[32m   1514\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to export the model to ONNX. Generating SARIF report at \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msarif_report_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1515\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSARIF is a standard format for the output of static analysis tools. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1518\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease report a bug on PyTorch Github: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_PYTORCH_GITHUB_ISSUES_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1519\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OnnxExporterError(\n\u001b[32m   1521\u001b[39m     ONNXProgram._from_failure(e, resolved_export_options.diagnostic_context),\n\u001b[32m   1522\u001b[39m     message,\n\u001b[32m   1523\u001b[39m ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOnnxExporterError\u001b[39m: Failed to export the model to ONNX. Generating SARIF report at 'report_dynamo_export.sarif'. SARIF is a standard format for the output of static analysis tools. SARIF logs can be loaded in VS Code SARIF viewer extension, or SARIF web viewer (https://microsoft.github.io/sarif-web-component/). Please report a bug on PyTorch Github: https://github.com/pytorch/pytorch/issues"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "onnx = torch.onnx.export(\n",
    "    model.variational_encoder,\n",
    "    torch.randn(1, 1, 343, device=\"cpu\"),\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}},\n",
    "    dynamo=True,\n",
    ")\n",
    "onnx.optimize()\n",
    "onnx.save(\"encoder.onnx\")\n",
    "\n",
    "onnx = torch.onnx.export(\n",
    "    model.decoder,\n",
    "    torch.randn(1, 3, device=\"cpu\"),\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}},\n",
    "    dynamo=True,\n",
    ")\n",
    "onnx.optimize()\n",
    "onnx.save(\"decoder.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the ONNX model with netron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q netron\n",
    "import netron\n",
    "netron.start('data/gaia/models/encoder.onnx', 8081)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
